{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55dccf8d4c03bba0b23eb02f560c8f38",
     "grade": false,
     "grade_id": "overview",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The Marshall Fire Destroyed More Structures than Any Fire In Colorado History\n",
    "\n",
    "\n",
    "![A neighborhood destroyed by the Marshall Fire](https://coloradonewsline.com/wp-content/uploads/2022/01/211231-POLIS-TOURS-WILDFIRES-HV-0602-1024x682.jpg)\n",
    "> Image source: https://coloradonewsline.com/briefs/1084-homes-louisville-superior-destroyed-marshall-fire-updated-tally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84870cded201b012b2c95438eafb3392",
     "grade": false,
     "grade_id": "ans-import",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'earthpy.appeears'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/thku8507.github.io/Notebooks/01-modis-ndvi.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bfuzzy-space-fortnight-v6vwj5965vq4h4p5/workspaces/thku8507.github.io/Notebooks/01-modis-ndvi.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bfuzzy-space-fortnight-v6vwj5965vq4h4p5/workspaces/thku8507.github.io/Notebooks/01-modis-ndvi.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mglob\u001b[39;00m \u001b[39mimport\u001b[39;00m glob\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bfuzzy-space-fortnight-v6vwj5965vq4h4p5/workspaces/thku8507.github.io/Notebooks/01-modis-ndvi.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mearthpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mappeears\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39metapp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bfuzzy-space-fortnight-v6vwj5965vq4h4p5/workspaces/thku8507.github.io/Notebooks/01-modis-ndvi.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfolium\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bfuzzy-space-fortnight-v6vwj5965vq4h4p5/workspaces/thku8507.github.io/Notebooks/01-modis-ndvi.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgeopandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'earthpy.appeears'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "import earthpy.appeears as etapp\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "\n",
    "# Set up logging so AppeearsDownloader will log in notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Ignore FutureWarning coming from hvplot\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8458730c591ff7b8e14b8dbe30f7cdf",
     "grade": false,
     "grade_id": "ans-datadir",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/earth-analytics/data/marshall-fire'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating path to project directory\n",
    "project_dir = os.path.join(\n",
    "    pathlib.Path.home(), 'earth-analytics', 'data', 'marshall-fire')\n",
    "\n",
    "# Create project directory\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "project_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f99f94324bc912c97d666491126d70e",
     "grade": false,
     "grade_id": "instr-boundary",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "&#128187; YOUR TASK:\n",
    "  * Head to the [Wildland Fire Interagency Geospatial Services (WFIGS) Historic Perimeters 2018 API explorer site](https://data-nifc.opendata.arcgis.com/datasets/nifc::historic-perimeters-2018/api) from the National Interagency Fire Center.\n",
    "  * Add filters until you get the single, final, perimeter. You may wish to try requesting the results in Python rather than on the API site since it will be easier to see how many records you get. One example that works:\n",
    "    * incidentname like 'CAMP'\n",
    "    * latest like 'Y'\n",
    "  * Paste the link in the code cell below and **assign it to a descriptive Python name**\n",
    "  * **Load the data into Python** using the `geopandas` library, e.g.:\n",
    "\n",
    "  ```python\n",
    "  gpd.read_file(url)\n",
    "  ```\n",
    "  \n",
    "  * Call your `GeoDataFrame` name at the end of the cell for testing\n",
    "  \n",
    "  > `geopandas.GeoDataFrame` is an extension of `pandas.DataFrame` that contains spatial and geometric information and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29861ba80f49a8b871ccf4eafe044859",
     "grade": false,
     "grade_id": "ans-boundary",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:fiona._env:Failed to read GeoJSON data\n"
     ]
    },
    {
     "ename": "DriverError",
     "evalue": "Failed to read GeoJSON data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: Failed to read GeoJSON data",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/thku8507.github.io/Notebooks/01-modis-ndvi.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bfuzzy-space-fortnight-v6vwj5965vq4h4p5/workspaces/thku8507.github.io/Notebooks/01-modis-ndvi.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m gpd\u001b[39m.\u001b[39;49mread_file(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://services3.arcgis.com/T4QMspbfLg3qTGWY/arcgis/rest/services/Historic_Geomac_Perimeters_2018/FeatureServer/0/query?where=incidentname\u001b[39;49m\u001b[39m%20%\u001b[39;49;00m\u001b[39m3D\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m20\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMARSHALL\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m20AND\u001b[39;49m\u001b[39m%20i\u001b[39;49;00m\u001b[39mncidentname\u001b[39;49m\u001b[39m%20%\u001b[39;49;00m\u001b[39m3D\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m20\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m&outFields=agency,active,datecurrent,uniquefireidentifier,fireyear,incidentname,pooownerunit,perimeterdatetime,gisacres,complexname,firecode,complexparentirwinid,pooresponsibleunit,state,inciwebid,localincidentidentifier,incomplex,complexfirecode,mergeid,latest,modifiedon,createdon,temp,shape__Area,shape__Length&outSR=4326&f=json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/geopandas/io/file.py:281\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         path_or_bytes \u001b[39m=\u001b[39m filename\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_file_fiona(\n\u001b[1;32m    282\u001b[0m         path_or_bytes, from_bytes, bbox\u001b[39m=\u001b[39;49mbbox, mask\u001b[39m=\u001b[39;49mmask, rows\u001b[39m=\u001b[39;49mrows, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munknown engine \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/geopandas/io/file.py:322\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     reader \u001b[39m=\u001b[39m fiona\u001b[39m.\u001b[39mopen\n\u001b[1;32m    321\u001b[0m \u001b[39mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mwith\u001b[39;00m reader(path_or_bytes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m features:\n\u001b[1;32m    323\u001b[0m         crs \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mcrs_wkt\n\u001b[1;32m    324\u001b[0m         \u001b[39m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiona/collection.py:783\u001b[0m, in \u001b[0;36mBytesCollection.__init__\u001b[0;34m(self, bytesbuf, **kwds)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_file \u001b[39m=\u001b[39m buffer_to_virtual_file(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbytesbuf, ext\u001b[39m=\u001b[39mext)\n\u001b[1;32m    782\u001b[0m \u001b[39m# Instantiate the parent class.\u001b[39;00m\n\u001b[0;32m--> 783\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvirtual_file, vsi\u001b[39m=\u001b[39;49mfiletype, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    784\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiona/collection.py:243\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m Session()\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mstart(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    244\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    245\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:588\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:143\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: Failed to read GeoJSON data"
     ]
    }
   ],
   "source": [
    "gpd.read_file(\"https://services3.arcgis.com/T4QMspbfLg3qTGWY/arcgis/rest/services/Historic_Geomac_Perimeters_2018/FeatureServer/0/query?where=incidentname%20%3D%20'MARSHALL'%20AND%20incidentname%20%3D%20'Y'&outFields=agency,active,datecurrent,uniquefireidentifier,fireyear,incidentname,pooownerunit,perimeterdatetime,gisacres,complexname,firecode,complexparentirwinid,pooresponsibleunit,state,inciwebid,localincidentidentifier,incomplex,complexfirecode,mergeid,latest,modifiedon,createdon,temp,shape__Area,shape__Length&outSR=4326&f=json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23f3d8dae0897d17469f10ec34b4eb6d",
     "grade": true,
     "grade_id": "tests-boundary",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ans_gdf = _\n",
    "gdf_pts = 0\n",
    "\n",
    "if isinstance(ans_gdf, gpd.GeoDataFrame):\n",
    "    print('\\u2705 Great work! You downloaded and opened a GeoDataFrame')\n",
    "    gdf_pts +=2\n",
    "else:\n",
    "    print('\\u274C Hmm, your answer is not a GeoDataFrame')\n",
    "\n",
    "if len(ans_gdf)==1:\n",
    "    print('\\u2705 Great work! You selected a single fire')\n",
    "    gdf_pts +=2\n",
    "else:\n",
    "    print('\\u274C Hmm, your GeoDataFrame does not have the right length')\n",
    "\n",
    "if ans_gdf.incidentname.iloc[0]=='CAMP' and ans_gdf.latest.iloc[0]=='Y':\n",
    "    print('\\u2705 Great work! You selected the Camp fire perimeter')\n",
    "    gdf_pts +=2\n",
    "else:\n",
    "    print('\\u274C Hmm, that is not the latest Camp fire boundary')\n",
    "    \n",
    "print('\\u27A1 You earned {} of 5 points for downloading a fire boundary'\n",
    "      .format(gdf_pts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad52b957923834c7ce35a240225f9574",
     "grade": false,
     "grade_id": "task-map",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Site Map\n",
    "\n",
    "The code below will help you to draw the Tribal subdivision boundaris on an interactive map.\n",
    "\n",
    "&#128187; Your task:\n",
    "  * Ask ChatGPT [how to plot a shapefile on a folium map](https://chat.openai.com/share/25988d5d-b355-4537-b2a1-71eafc60f67e)\n",
    "  * Adapt the code to use the boundary data you downloaded\n",
    "  * Add a labeled marker for Paradise, CA\n",
    "  * Center the map at that same location with a reasonable `zoom_start` level\n",
    "\n",
    "> **GOTCHA ALERT:** Make sure to call your map at the end of the cell so that it will display in your Notebook\n",
    "\n",
    "&#127798; Customize your plot - can you add ESRI World Imagery as the basemap/background?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "feae0ca9af096d0a8b031515fefd5993",
     "grade": false,
     "grade_id": "ans-map",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "564aea800165de24ed49e89c811937e9",
     "grade": false,
     "grade_id": "step-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## STEP 3: DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60a44e71e9c444dfe9963c9fa31fccc7",
     "grade": false,
     "grade_id": "instr-ndvi",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Observing vegetation health from space\n",
    "We will look at the destruction and recovery of vegetation in the area using the summertime (peak green) Normalized Difference Vegetation Index (NDVI). How does it work? First, we need to learn about spectral reflectance signatures.\n",
    "\n",
    "Every object reflects some wavelengths of light more or less than others. We can see this with our eyes, since, for example, plants reflect a lot of green in the summer, and then as that green diminishes in the fall they look more yellow or orange. The image below shows spectral signatures for water, soil, and vegetation:\n",
    "\n",
    "![](https://seos-project.eu/remotesensing/images/Reflexionskurven.jpg)\n",
    "> Image source: [SEOS Project](https://seos-project.eu/remotesensing/remotesensing-c01-p06.html)\n",
    "\n",
    "Healthy vegetation reflects a lot of Near-InfraRed (NIR) radiation. Less healthy vegetation reflects a similar amounts of the visible light spectra, but less NIR radiation. We don't see a huge drop in Green radiation until the plant is very stressed or dead. That means that NIR allows us to get ahead of what we can see with our eyes.\n",
    "\n",
    "![](https://camo.githubusercontent.com/176b39433ef30866421bf28988812852b94a7705c75764deebde846560aebbc6/68747470733a2f2f666c75726f7361742e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f31302f67726f7774682d6d6f6e69746f72696e672d65313538333930323232383836372e706e6729)\n",
    "> Image source: [Spectral signature literature review by px39n](https://github.com/px39n/Awesome-Vegetation-Index)\n",
    "\n",
    "Different species of plants reflect different spectral signatures, but the *pattern* of the signatures are similar. NDVI compares the amount of NIR reflectance to the amount of Red reflectance, thus accounting for many of the species differences and isolating the health of the plant. The formula for calculating NDVI is:\n",
    "\n",
    "$$NDVI = \\frac{(NIR - Red)}{(NIR + Red)}$$\n",
    "\n",
    "&#128214; Read more about NDVI and other vegetation indices:\n",
    "  * [earthdatascience.org](https://www.earthdatascience.org/courses/use-data-open-source-python/multispectral-remote-sensing/vegetation-indices-in-python/calculate-NDVI-python/)\n",
    "  * [USGS](https://www.usgs.gov/landsat-missions/landsat-surface-reflectance-derived-spectral-indices)\n",
    "\n",
    "You will download NDVI data collected from the MODIS platform for the study period. MODIS is a multispectral instrument that measures Red and NIR data (and so can be used for NDVI). There are two MODIS sensors on two different platforms: satellites Terra and Aqua.\n",
    "\n",
    "&#128214; [Learn more about MODIS datasets and science](https://modis.gsfc.nasa.gov/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "443b830b01a886c5270d665a70a91592",
     "grade": false,
     "grade_id": "task-cite",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "&#9998; In the cell below, write a description of the data you will use. Make sure to include:\n",
    "  * A citation\n",
    "  * A brief explanation of the platform (Aqua satellite), sensor (MODIS), and post-processing (NDVI), including why these data will help you see vegetation recovery after a wildfire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR DATA DESCRIPTION AND CITATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90d490751417061e56d707ae000519d7",
     "grade": false,
     "grade_id": "instr-appdownload",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exploring the AppEEARS API for NASA Earthdata access\n",
    "\n",
    "We're going to ask for a special download that only covers our study area, so we can't just find a link to the data - we have to negotiate with the data server. We're doing this using the [APPEEARS](https://appeears.earthdatacloud.nasa.gov/) API (Application Programming Interface). The API makes it possible for you to request data using code. Luckily, the `earthpy` library you imported can complete the entire negotiation, including picking up where you left of downloading should you have to restart your kernel.\n",
    "\n",
    "> HINT: In order to download APPEEARS data, you will need and Earthdata account. `earthpy` will prompt you to store your username and password in your system keyring.\n",
    "\n",
    "&#128187; YOUR TASK: \n",
    "1. In the cell below, paste the following code as a starting point:\n",
    "\n",
    "    ```python\n",
    "    # Initialize AppeearsDownloader for MODIS NDVI data\n",
    "    ndvi_downloader = etapp.AppeearsDownloader(\n",
    "        download_key='modis-ndvi',\n",
    "        ea_dir=project_dir,\n",
    "        product='',\n",
    "        layer='',\n",
    "        start_date='',\n",
    "        end_date='',\n",
    "        recurring=,\n",
    "        year_range=,\n",
    "        polygon=gdf\n",
    "    )\n",
    "    \n",
    "    # Download files if the download directory does not exist\n",
    "    if not os.path.exists(ndvi_downloader.data_dir):\n",
    "        ndvi_downloader.download_files()\n",
    "    \n",
    "    ndvi_downloader\n",
    "    ```\n",
    "2. Modify `project_dir` to match **your** project directory name, and `gdf` to match the name of **your** `GeoDataFrame`.\n",
    "3. Fill out the parameters so that you are downloading the highest **available** resolution **NDVI** layer of the **Aqua MODIS** platform from **Summer months (June, July, August) of 2017 to 2022**.\n",
    "\n",
    "  > **HINT**: You will need to consult with the [list of APPEEARS datasets](https://appeears.earthdatacloud.nasa.gov/products).\n",
    "\n",
    "  > **GOTCHA ALERT**: The product name will need to be formatted as `<product-name>.<version>`\n",
    "\n",
    "4. Run the code. Note that orders from the APPEEARS API can take a few hours to be delivered. Check your status by logging in at [the Appeears status page](https://appeears.earthdatacloud.nasa.gov/explore)\n",
    "   \n",
    "   > **GOTCHA ALERT**: If your download fails, but your download directory has been created, this code will not initialize a new download. To start over fresh, delete the download directory in the Terminal OR change the `download_key`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e316f977498770de1930aa52deb79db7",
     "grade": false,
     "grade_id": "ans-appdownload",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bec50593d9e07386ed12d0401b342de7",
     "grade": true,
     "grade_id": "tests-appdownload",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR WORK - DO NOT MODIFY!\n",
    "ans_download = _\n",
    "download_pts = 0\n",
    "\n",
    "if os.path.exists(ndvi_downloader.data_dir):\n",
    "    print('\\u2705 Great work! You started your download')\n",
    "    download_pts += 1\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like your data did not download')\n",
    "\n",
    "if len(glob(os.path.join(ndvi_downloader.data_dir, '*', '*')))==82:\n",
    "    print('\\u2705 Great work! Your download succeeded')\n",
    "    download_pts += 4\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like your data did not download')\n",
    "\n",
    "# Subtract one point for any PEP-8 errors\n",
    "tmp_path = \"tmp.py\"\n",
    "with open(tmp_path, \"w\") as tmp_file:\n",
    "    tmp_file.write(In[-2])\n",
    "ignore_flake8 = 'W292,F401,E302,F821'\n",
    "flake8_out = subprocess.run(\n",
    "    ['flake8', \n",
    "     '--ignore', ignore_flake8, \n",
    "     '--import-order-style', 'edited',\n",
    "     '--count', \n",
    "     tmp_path],\n",
    "    stdout=subprocess.PIPE,\n",
    ").stdout.decode(\"ascii\")\n",
    "print(flake8_out)\n",
    "download_pts -= int(flake8_out.splitlines()[-1])\n",
    "\n",
    "print('\\u27A1 You earned {} of 5 points for downloading data'\n",
    "      .format(download_pts))\n",
    "\n",
    "download_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9fc19a0b63a64bc0f78048c0039ebd1",
     "grade": false,
     "grade_id": "instr-glob",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Putting it together: Working with multi-file raster datasets in Python\n",
    "\n",
    "Now you need to load all the downloaded files into Python. Let's start by getting all the file names. You will also need to extract the date from the filename. Check out [the lesson on getting information from filenames in the textbook](https://www.earthdatascience.org/courses/intro-to-earth-data-science/write-efficient-python-code/loops/data-workflows-with-loops/).\n",
    "\n",
    "> **GOTCHA ALERT:** `glob` doesn't necessarily find files in the order you would expect. Make sure to **sort** your file names like it says in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1bc37e1716354b4b386886e61fa2fa9",
     "grade": false,
     "grade_id": "ans-glob",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83d238ebec333a7c4584605b11fe107a",
     "grade": true,
     "grade_id": "tests-glob",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE - DO NOT MODIFY!\n",
    "ans_glob = _\n",
    "glob_pts = 0\n",
    "\n",
    "if len(ans_glob)==41:\n",
    "    print('\\u2705 Great work! Your correctly filtered your files')\n",
    "    glob_pts += 5\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like you did not find the correct files')\n",
    "\n",
    "# Subtract one point for any PEP-8 errors\n",
    "tmp_path = \"tmp.py\"\n",
    "with open(tmp_path, \"w\") as tmp_file:\n",
    "    tmp_file.write(In[-2])\n",
    "ignore_flake8 = 'W292,F401,E302,F821'\n",
    "flake8_out = subprocess.run(\n",
    "    ['flake8', \n",
    "     '--ignore', ignore_flake8, \n",
    "     '--import-order-style', 'edited',\n",
    "     '--count', \n",
    "     tmp_path],\n",
    "    stdout=subprocess.PIPE,\n",
    ").stdout.decode(\"ascii\")\n",
    "print(flake8_out)\n",
    "glob_pts -= int(flake8_out.splitlines()[-1])\n",
    "\n",
    "print('\\u27A1 You earned {} of 5 points for getting file names'\n",
    "      .format(glob_pts))\n",
    "\n",
    "glob_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 - IMPORT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e37bef6624f7a538cbc2ca155d66af00",
     "grade": false,
     "grade_id": "instr-load",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Repeating tasks in Python\n",
    "\n",
    "Now you should have dozens of files! For each file, you need to:\n",
    "  * Load the file in using the `rioxarray` library\n",
    "  * Get the date from the file name\n",
    "  * Add the date as a dimension coordinate\n",
    "  * Give your data variable a name\n",
    "  * Divide by the scale factor of 10000\n",
    "\n",
    "You don't want to write out the code for each file -- That's a recipe for copy pasta. Luckily, Python has tools for doing similar tasks repeatedly. In this case, you'll use one called a `for` loop.\n",
    "\n",
    "Check out the [textbook page on `for` loops](https://www.earthdatascience.org/courses/intro-to-earth-data-science/write-efficient-python-code/loops/automate-data-tasks-with-loops/)\n",
    "\n",
    "There's some code below that uses a `for` loop in what is called an **accumulation pattern** to process each file. That means that you will save the results of your processing to a list each time you process the files, and then merge all the arrays in the list. \n",
    "\n",
    "```python\n",
    "ndvi_das = []\n",
    "for ndvi_path in ndvi_paths:\n",
    "    # Get date from file name\n",
    "    doy = ndvi_path[doy_start:doy_end]\n",
    "    date = pd.to_datetime(doy, format='')\n",
    "\n",
    "    # Open dataset\n",
    "    da = rxr.open_rasterio(ndvi_path, masked=True).squeeze()\n",
    "\n",
    "    # Prepare to concatenate: Add date dimension and clean up metadata\n",
    "    da = da.assign_coords({'date': date})\n",
    "    da = da.expand_dims({'date': 1})\n",
    "    da.name = 'NDVI'\n",
    "\n",
    "    # Divide by scale factor\n",
    "\n",
    "    # Add the DataArray to the end of the accumulator list\n",
    "\n",
    "ndvi_das\n",
    "```\n",
    "\n",
    "Your task is to:\n",
    "  1. Replace any names with your chosen variable names\n",
    "  2. Look at the file names. How many characters from the end is the beginning of the date? The end of the date? Define the missing names `doy_start`, `doy_end`.\n",
    "  3. Assign the name `scale_factor` the correct value for this NDVI dataset (HINT: NDVI should range between 0 and 1)\n",
    "  4. Put the correct **format string** for this application in the `pd.to_datetime` function\n",
    "  5. Add the code needed to divide by the scale factor and stick the now-loaded DataArray at the end of your list of DataArrays (accumulate).\n",
    "  \n",
    "&#127798; You can also get the date with a tool called a regular expression (`re` library). See if you can get the code working that way too! You may find it helpful to test your regular expression at this [regex tester site](https://regex101.com/).\n",
    "\n",
    "> HINT: **DO NOT** try to complete all these steps without running your code! Use these debugging tips:\n",
    ">   * Comment out code you haven't gotten to yet\n",
    ">   * Complete one step at a time\n",
    ">   * Test your work by looking at the results before moving on to the next step\n",
    ">   * Use the `break` keyword at the end of your `for` loop (or a breakpoint) to run the code on only the first `DataArray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ce8dee4bbb7b1639f031f6642e396d1",
     "grade": false,
     "grade_id": "ans-load",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62c0a761f09e30e9985b0e4908382d6e",
     "grade": true,
     "grade_id": "tests-load",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR WORK - DO NOT MODIFY!\n",
    "ans_open = _\n",
    "open_pts = 0\n",
    "\n",
    "# Check that the called value is a list of DataArrays\n",
    "if all([isinstance(da, xr.DataArray) for da in ans_open]):\n",
    "    print('\\u2705 Great work! You loaded your data into DataArrays')\n",
    "    open_pts += 2\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like you have not loaded your data yet')\n",
    "\n",
    "# Check that there are the right number of DataArrays\n",
    "if len(ans_open)==41:\n",
    "    print('\\u2705 Great work! Your loaded all the DataArrays')\n",
    "    open_pts += 2\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like you have too few or too many DataArrays')\n",
    "\n",
    "# Check that there is a datetime coordinate\n",
    "has_dt_coord_list = []\n",
    "for da in ans_open:\n",
    "    has_dt_coord = False\n",
    "    for name, coord in da.coords.items():\n",
    "        try:\n",
    "            coord.dt\n",
    "            has_dt_coord = True\n",
    "        except TypeError:\n",
    "            pass\n",
    "    has_dt_coord_list.append(has_dt_coord)\n",
    "if all(has_dt_coord_list):\n",
    "    print('\\u2705 Great work! You added a Datetime Coordinate')\n",
    "    open_pts += 4\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like you did not add a date coordinate '\n",
    "          'or you did not correctly convert your dates')\n",
    "\n",
    "# Check that the scale factor was applied\n",
    "if (all([(da.max() <= 1) for da in ans_open]) \n",
    "        and all([(da.min() >= -1) for da in ans_open])):\n",
    "    print('\\u2705 Great work! You correctly scaled your data')\n",
    "    open_pts += 2\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like you did not scale your data')\n",
    "\n",
    "# Subtract one point for any PEP-8 errors\n",
    "tmp_path = \"tmp.py\"\n",
    "with open(tmp_path, \"w\") as tmp_file:\n",
    "    tmp_file.write(In[-2])\n",
    "ignore_flake8 = 'W292,F401,E302,F821'\n",
    "flake8_out = subprocess.run(\n",
    "    ['flake8', \n",
    "     '--ignore', ignore_flake8, \n",
    "     '--import-order-style', 'edited',\n",
    "     '--count', \n",
    "     tmp_path],\n",
    "    stdout=subprocess.PIPE,\n",
    ").stdout.decode(\"ascii\")\n",
    "print(flake8_out)\n",
    "open_pts -= int(flake8_out.splitlines()[-1])\n",
    "\n",
    "print('\\u27A1 You earned {} of 10 points for downloading data'\n",
    "      .format(open_pts))\n",
    "\n",
    "open_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4263cf7ddc7398e91dd3990d8680a4fd",
     "grade": false,
     "grade_id": "instr-combine",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, stack your arrays by date into a time series using the `xr.combine_by_coords()` function. You will have to tell it which **dimension** you want to stack your data in, using the `coords=['dimension_name']` parameter.\n",
    "\n",
    "> HINT: which dimension do you want to get longer?\n",
    "\n",
    "> GOTCHA ALERT: The `coords` parameter must be a list, even if there is only one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7d5f163492e6826cc683156300e6d8b",
     "grade": false,
     "grade_id": "ans-combine",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b045a446306fa6a12b88f7b36dcfa011",
     "grade": true,
     "grade_id": "tests-combine",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR WORK - DO NOT MODIFY!\n",
    "ans_combine = _\n",
    "combine_pts = 0\n",
    "\n",
    "# Check that the called value is a Dataset\n",
    "if isinstance(ans_combine, xr.Dataset):\n",
    "    print('\\u2705 Great work! You combined your data into a Dataset')\n",
    "    combine_pts += 1\n",
    "else:\n",
    "    print('\\u274C Hmm, did you call your combined Dataset?')\n",
    "\n",
    "# Check that there are the right number of DataArrays\n",
    "if ans_combine.dims=={'x': 205, 'y': 144, 'date': 41}:\n",
    "    print('\\u2705 Great work! Your loaded all the DataArrays')\n",
    "    combine_pts += 2\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like you have too few or too many '\n",
    "          'DataArrays')\n",
    "\n",
    "# Subtract one point for any PEP-8 errors\n",
    "tmp_path = \"tmp.py\"\n",
    "with open(tmp_path, \"w\") as tmp_file:\n",
    "    tmp_file.write(In[-2])\n",
    "ignore_flake8 = 'W292,F401,E302,F821'\n",
    "flake8_out = subprocess.run(\n",
    "    ['flake8', \n",
    "     '--ignore', ignore_flake8, \n",
    "     '--import-order-style', 'edited',\n",
    "     '--count', \n",
    "     tmp_path],\n",
    "    stdout=subprocess.PIPE,\n",
    ").stdout.decode(\"ascii\")\n",
    "print(flake8_out)\n",
    "combine_pts -= int(flake8_out.splitlines()[-1])\n",
    "\n",
    "print('\\u27A1 You earned {} of 3 points for downloading data'\n",
    "      .format(combine_pts))\n",
    "\n",
    "combine_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b332a29ef59bbb5893eda758d7c5e8f7",
     "grade": false,
     "grade_id": "task-plot-map",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Plot the change in NDVI spatially\n",
    "\n",
    "&#128187; **YOUR TASK:** Complete the following (ask ChatGPT or look in the xarray documentation if you aren't sure how to complete the first 4 steps):\n",
    "  1. Select data from 2019 using the `.sel()` method\n",
    "  2. Take the temporal mean (over the **date**, not spatially) using the `.mean()` method\n",
    "  3. Repeat for the data from 2017\n",
    "  4. Subtract the pre-fire year from the post-fire years\n",
    "  5. Plot the result\n",
    "\n",
    "> **HINT**: You can use the `hvplot()` method with an `xarray.Dataset` too! When plotting maps, there are many considerations related to the type of geographic coordinates you are using. `hvplot` can take care of a lot of that for you, provided that the `geoviews` package is installed (which it should be for you). However, to use the plotting methods that avoid a warped image you **must** include the parameter `geo=True`, as well as specifying `x='x'` and `y='y'`.\n",
    "    \n",
    "  6. Use a **diverging** color map, e.g. by chaining `.opts(cmap='PiYG')` onto the end of your spatial plotting code.\n",
    "\n",
    "> There are different types of color maps for different types of data. In this case, we want decreases to be a different color from increases, so we use a **diverging** color map. Check out available colormaps in the [matplotlib documentation](https://matplotlib.org/stable/tutorials/colors/colormaps.html).\n",
    "\n",
    "&#127798; For an extra challenge, add the Camp Fire Boundary to the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c88e92d6e3ac19209c8600e85a03b606",
     "grade": false,
     "grade_id": "ans-plot-map",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd70cefd09be91d633d6524b00b1eb41",
     "grade": false,
     "grade_id": "instr-out",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Did the NDVI drop more inside the fire boundary than nearby?\n",
    "\n",
    "You will compute the mean NDVI inside and outside the fire boundary. I have tried asking ChatGPT how to accomplish this [in this chat](https://chat.openai.com/share/ea0a406d-7344-4bd4-ba66-f25e82e6a9fb). I got a useful response, but I wasn't able to get a workflow from ChatGPT that did not require the **envelope** of the fire boundary as a separate file.\n",
    "\n",
    "> NOTE: An **envelope** of a shape is the smallest rectangle that contains the entire shape.\n",
    "\n",
    "&#128187; **YOUR TASK:**\n",
    "  1. First, use the code below to get the envelope of the fire boundary as a `GeoDataFrame`:\n",
    "\n",
    "    ```python\n",
    "    gpd.GeoDataFrame(geometry=gdf.envelope)\n",
    "    ```\n",
    "    \n",
    "  2. Use the code from ChatGPT as a starting point, or writing your own, get the geometric difference between the envelope and the fire boundary as a `GeoDataFrame`\n",
    "\n",
    "  3. Test if the geometry was modified correctly -- Add some code to help you take a look at the results.\n",
    "\n",
    "> HINT: You can use the `hvplot()` method to plot `GeoDataFrame`s too, so you can check that the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b2a49637bcc563394fc7aef8d5bcb2b",
     "grade": false,
     "grade_id": "ans-out",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63cd54acdb2839305e1e8d2f475feed7",
     "grade": true,
     "grade_id": "tests-out",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE - DO NOT MODIFY!\n",
    "ans_out = _\n",
    "out_pts = 0\n",
    "\n",
    "if int(round(out_gdf.to_crs(26910).area, -4))==589960000:\n",
    "    print('\\u2705 Great work! Your correctly took a spatial difference')\n",
    "    out_pts += 5\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like your spatial difference is incorrect')\n",
    "\n",
    "# Subtract one point for any PEP-8 errors\n",
    "tmp_path = \"tmp.py\"\n",
    "with open(tmp_path, \"w\") as tmp_file:\n",
    "    tmp_file.write(In[-2])\n",
    "ignore_flake8 = 'W292,F401,E302,F821'\n",
    "flake8_out = subprocess.run(\n",
    "    ['flake8',\n",
    "     '--ignore', ignore_flake8,\n",
    "     '--import-order-style', 'edited',\n",
    "     '--count',\n",
    "     tmp_path],\n",
    "    stdout=subprocess.PIPE,\n",
    ").stdout.decode(\"ascii\")\n",
    "print(flake8_out)\n",
    "out_pts -= int(flake8_out.splitlines()[-1])\n",
    "\n",
    "print('\\u27A1 You earned {} of 5 points for getting file names'\n",
    "      .format(out_pts))\n",
    "\n",
    "out_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ee442ad9e0cb2a6fd0bff71957d18d0",
     "grade": false,
     "grade_id": "instr-clip",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "&#128187; **YOUR TASK:** Clip your DataArray to the boundaries for both inside and outside the reservation. You will need to replace the `GeoDataFrame` name with your own. Check out the [lesson on clipping data with the `rioxarray` library in the textbook](https://www.earthdatascience.org/courses/use-data-open-source-python/intro-raster-data-python/raster-data-processing/crop-raster-data-with-shapefile-in-python/).\n",
    "\n",
    "> **GOTCHA ALERT:** It's important to use `from_disk=True` when clipping large arrays like this. It allows the computer to use less valuable memory resources when clipping - you will probably find that otherwise the cell below crashes the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "858af57583a902c647353720fbff3fb2",
     "grade": false,
     "grade_id": "ans-clip",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e91fbe0cd454111c70ab49f5a14d655",
     "grade": false,
     "grade_id": "task-diff",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "&#128187; **YOUR TASK:** Practice chaining `xarray` methods! For **both inside and outside** the Camp fire boundary:\n",
    "  1. Group the data by year using\n",
    "  2. Take the mean. You always need to tell reducing methods in `xarray` what dimensions you want to reduce. When you want to summarize data across **all** dimensions (except the group dimension(s)), you can use the `...` syntax, e.g. `.mean(...)` as a shorthand.\n",
    "  3. Select the NDVI variable\n",
    "  4. Convert to a `DataFrame` using the `to_dataframe()` method\n",
    "  5. Call **both** of your `DataFrame`s at the end of the cell for testing, e.g.\n",
    "     ```python\n",
    "     inside_df, outside_df\n",
    "     ```\n",
    "\n",
    "> **GOTCHA ALERT:** the DateIndex in pandas is a little different from the Datetime Dimension in xarray. You will need to use the `.dt.year` syntax to access information about the year, not just `.year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc2e3e3da18ba398dc1ad2b6ec04c637",
     "grade": false,
     "grade_id": "ans-diff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "026c6716dbb109e20c1929cb68b4d21a",
     "grade": true,
     "grade_id": "tests-diff",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE - DO NOT MODIFY!\n",
    "print(_)\n",
    "(ans_in, ans_out) = _\n",
    "out_pts = 0\n",
    "\n",
    "if round((ans_in - ans_out).sum(), 2).sum()==-0.29:\n",
    "    print('\\u2705 Great work! Your correctly clipped your data')\n",
    "    out_pts += 5\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like your clipping is incorrect')\n",
    "\n",
    "# Subtract one point for any PEP-8 errors\n",
    "tmp_path = \"tmp.py\"\n",
    "with open(tmp_path, \"w\") as tmp_file:\n",
    "    tmp_file.write(In[-2])\n",
    "ignore_flake8 = 'W292,F401,E302,F821'\n",
    "flake8_out = subprocess.run(\n",
    "    ['flake8',\n",
    "     '--ignore', ignore_flake8,\n",
    "     '--import-order-style', 'edited',\n",
    "     '--count',\n",
    "     tmp_path],\n",
    "    stdout=subprocess.PIPE,\n",
    ").stdout.decode(\"ascii\")\n",
    "print(flake8_out)\n",
    "out_pts -= int(flake8_out.splitlines()[-1])\n",
    "\n",
    "print('\\u27A1 You earned {} of 5 points for clipping'\n",
    "      .format(out_pts))\n",
    "\n",
    "out_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9815a35a147261ba91724fe251abeed0",
     "grade": false,
     "grade_id": "task-diff-plot",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "&#128187; YOUR TASK:\n",
    "  1. Take the difference between outside and inside the Reservation\n",
    "     > GOTCHA ALERT: You may need to select the NDVI columns of your inside/outside `DataFrame`s in order to subtract effectively\n",
    "  2. Save it as a new column in your `DataFrame`\n",
    "  3. Plot the difference. What do you observe? Don't forget to write a **headline and description** of your plot!\n",
    "\n",
    "&#127798; For an extra challenge, add a vertical line showing when the fire occurred. Note that unfortunately you will need to convert the date to a decimal of a year. You may find the pandas datetime method `.toordinal()` useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ff499daec2699133a2be0e944db3077",
     "grade": false,
     "grade_id": "ans-diff-plot",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5519198e00659e5f4c7a56e81955ff1b",
     "grade": true,
     "grade_id": "headline",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec5e521a234f921e91f2e972192720d1",
     "grade": false,
     "grade_id": "instr-repeat-workflow",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Your turn! Repeat this workflow in a different time and place for your portfolio.\n",
    "\n",
    "It's not just water rights that affect NDVI! You could look at: \n",
    "  * Recovery after a national disaster, like a wildfire or hurricane\n",
    "  * The effects of drought on crop health\n",
    "  * Deforestation\n",
    "\n",
    "You can even choose a different dataset, like Landsat, and/or a different spectral index. [Check out some other ways to enhance images and highlight different phenomena](https://www.usgs.gov/landsat-missions/landsat-surface-reflectance-derived-spectral-indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
